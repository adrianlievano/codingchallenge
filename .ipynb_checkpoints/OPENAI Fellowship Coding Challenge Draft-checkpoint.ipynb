{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Adrian Lievano submission for OPENAI Coding Challenge </H1>\n",
    "\n",
    "This notebook breakdowns the process for extracting all of the usernames in a given database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = lambda prefix: [d for d in database if d.startswith(prefix)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> CODING CHALLENGE PROBLEM DESCRIPTION </H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're on a website (such as Github!) with a text field which autocompletes usernames as you type. Under the hood, there's an API call which takes in the prefix of a username and then returns all usernames which start with that prefix, lexicographically sorted and truncated at 5 results.\n",
    "\n",
    "Your task is to use this API call to dump the entire user database, specifically:\n",
    "\n",
    "Implement the extract function in autocomplete.py. extract should return the whole user database, making calls to query under the hood.\n",
    "\n",
    "Notes:\n",
    "\n",
    "You can assume all valid usernames are comprised of lowercase ASCII letters (a-z).\n",
    "Assume that queries to the API are expensive and should be minimized, but it's more important to have a correct answer and well-structured solution than an optimal answer.\n",
    "You are welcome to include any tests or documentation that you'd normally provide when checking in to a shared codebase.\n",
    "Submit your answer as a secret Gist. Please do not make your answers public (and please do not look for solutions from others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(query):\n",
    "    \"\"\"Implement this method using the `query` API call, for example:\n",
    "    query(\"abracadar\") #=> [\"abracadara\"] using the default query method in main()\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return [...]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Runs your solution -- no need to update (except to maybe change the database).\"\"\"\n",
    "    # Simple implementation of the autocomplete API\n",
    "    database = [\"abracadara\", \"al\", \"alice\", \"alicia\", \"allen\", \"alter\", \"altercation\", \"bob\", \"eve\", \"evening\", \"event\", \"eventually\", \"mallory\"]\n",
    "    query = lambda prefix: [d for d in database if d.startswith(prefix)][:5]\n",
    "    assert extract(query) == database\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Extraction Code </h1>\n",
    "\n",
    "The Code below extracts 1st, 2nd, and 3rd orders features (character combinations) that lead to identifying every username in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Rationale Behind the Algorithm </h1>\n",
    "\n",
    "This algorithm takes in letters, or caps, A-Z, a-z, numbers, special characters -- because usernames can be anything, right? Then, it proceeds to identify lower-order features to higher-order ones, systematically decreasing the database by removing usernames from the original database that are below the number of characters defined by a given filter. For example, in the string, 'Alter', 'A', 'Al', 'Alt', are 1st, 2nd, and 3rd order features, respectively.\n",
    "\n",
    "After extracting a feature, for example, 'A', it uses the discovered nth-features to query the dataset on the remaining_database from the (nth-1) feature.\n",
    "\n",
    "We're calling quieries on the set of identified 1st, 2nd, 3rd, ... nth order features (characters in a word) that is unique to our database. By adding checker flags between each feature-order, we can minimize the number of characters needed to identify all the users in the database, thus minimizing the number of times we have to call query API.\n",
    "\n",
    "If there wasn't a minimum, or a maximum, on the number of characters, I think we can continue to draw out the nth order of terms to selectively identify more complex usernames and extract more complex names and sized databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zakeno\n",
      "horuvu\n",
      "navomi\n",
      "zukeyo\n",
      "fibofu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import sys\n",
    "\n",
    "\n",
    "VOWELS = \"aeiou\"\n",
    "CONSONANTS = \"\".join(set(string.ascii_lowercase) - set(VOWELS))\n",
    "\n",
    "def generate_word(length):\n",
    "    word = \"\"\n",
    "    for i in range(length):\n",
    "        if i % 2 == 0:\n",
    "            word += random.choice(CONSONANTS)\n",
    "        else:\n",
    "            word += random.choice(VOWELS)\n",
    "    return word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        count = int(sys.argv[1])\n",
    "    except:\n",
    "        count = 5\n",
    "\n",
    "    try:\n",
    "        length = int(sys.argv[2])\n",
    "    except:\n",
    "        length = 6\n",
    "\n",
    "    for i in range(count):\n",
    "        print(generate_word(length))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adrian Lievano's CODE STARTS HERE\n",
    "#Set & Sorts database, in case it doesn't contain unique usernames or if it's not properly ordered\n",
    "#database = ['ASD','ASD','GOHASD','GOHAN','ASDD,','GTGTG','GTGsdsd','lo','adrian','guano','jimcarrey','grunt','MasterChief','AAAAAAAA','AAA','AAB','AABC','AAACC','AAACD','AAACVX','$','a','b','c','d',\"abracadara\", \"al\", \"alice\",\"#312Yomama\",\"#412Yomama\",\"#512Yomama\",\"#612Yomama\",\"#712Yomama\",\"#812Yomama\", \"alicia\", \"allen\", \"alter\",'913KILLA','123Banyo','xx#Chaosxx#','23Yawwhey' \"altercation\",\"amap\",\"ammap\",\"azap\",\"azzopa\",\"AcePilota19993\",\"bob\", \"bzaop\", \"eve\", \"evening\", \"event\", \"eventually\", \"mallory\"]\n",
    "import random\n",
    "import string\n",
    "N=500\n",
    "#database = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(N))\n",
    "\n",
    "#database = ['%%%%%%%','%%%%%%%%%%%','vtavatv','yhyrhrt','tujtujtjy','etyjetyjetyj','etyjetyjetyj','%%%','myumyum','$$$$$$$','$$$$$$$$$','$$$$$$$$$$$$','$$$$$$$$$2','AS','ASDFASDF','TGRTRTH','RTHRTHRTHRT','ASDGSDHSG','yjujtyjtyj','ytytydnyt','dsvfasdfva','asvasdvasdvy','rgrgerg','ergergerg','vasdvasdvasdvdvadsvasdvasdvasdsdvasd','ynysrnsnfgnsfgnsfgn','$$$$','ADGAFBYY','BYSBSB','asdasda','asdvasdvtthddbtdtbdbdtbdtb','ASD','ASD','GOHASD','GOHAN','ASDD,','GTGTG','GTGsdsd','lo','adrian','guano','jimcarrey','grunt','MasterChief','AAAAAAAA','AAA','AAB','AABC','AAACC','AAACD','AAACVX','$','a','b','c','d',\"abracadara\", \"al\", \"alice\",\"#312Yomama\",\"#412Yomama\",\"#512Yomama\",\"#612Yomama\",\"#712Yomama\",\"#812Yomama\", \"alicia\", \"allen\", \"alter\",'913KILLA','123Banyo','xx#Chaosxx#','23Yawwhey' \"altercation\",\"amap\",\"ammap\",\"azap\",\"azzopa\",\"AcePilota19993\",\"bob\", \"bzaop\", \"eve\", \"evening\", \"event\", \"eventually\", \"mallory\"]\n",
    "database=[]\n",
    "for i in range(100):\n",
    "    database.append(generate_word(15))\n",
    "    \n",
    "database = sorted(set(database))\n",
    "\n",
    "def getWords(data):\n",
    "    words=[]\n",
    "    for i in range(len(data)):\n",
    "        word=data[i]\n",
    "        words.append(word)\n",
    "    return words\n",
    "    \n",
    "   ## for strings in range(len(word.split())):\n",
    "      #  #print(strings)\n",
    "    \n",
    "def getLetters(allWords):\n",
    "    \n",
    "    #Initialize chars and Letters matrix\n",
    "    chars = []    \n",
    "    fls=[]\n",
    "    for x in range(len(allWords)):\n",
    "        listChar=list(allWords[x])\n",
    "        chars.append(listChar)   \n",
    "    #return chars\n",
    "\n",
    "    for i in range(len(allWords)):\n",
    "        fl=chars[i][0:]\n",
    "        fls.append(fl)\n",
    "    return fls\n",
    "\n",
    "#def check_ifs (fls):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "Remaining usernames: ['bacumepafomujec', 'bawituzanutigil', 'bazewavoyetuyix', 'behefitiqoqenex', 'bemopodevuyufec', 'botinarezofuzit', 'bufavilisiqawol', 'cawekizozarahod', 'cazabugucizegul', 'cufumegatacajaf', 'dizubedasenabep', 'duromijuposemul', 'favapopamepomag', 'fegeyenawulevoz', 'fegibogafizihop', 'fenuleteyutijus', 'filalibixexobur', 'fokedacogasadog', 'gamonixuruxotof', 'gijacemanodepas', 'guqogecocagilir', 'hicoqeniluhacov', 'hisefipesamijod', 'hivorukalowaxup', 'hobofevobasicav', 'hojabugiwanoful', 'hokayozojahumen', 'jobuyivetacekiy', 'jupojecajeqomex', 'kajevehaneratad', 'kiduhecowemuhuy', 'kinuwegumoxipig', 'kirafuyavegehic', 'kirikuwulirutip', 'kofokuvawewomow', 'kududexalodakuw', 'kujosibahudagef', 'kuxoxiwojiquhek', 'laraqoxugasoniq', 'lelarepekeriwim', 'lexiruducuwazod', 'liralefezilaxot', 'lizotikayiserir', 'mojexenavexijas', 'molititoqokaxit', 'namitayavifuhod', 'nasoyicahehocib', 'nedadolemanaziv', 'nenuxokaqosidih', 'nihofuyofiyaxuj', 'nirilobalosoloz', 'niyajegilamikig', 'nuduvilurakawex', 'nunoqidozanetal', 'pamigazidopupop', 'pavizobusadevez', 'paxinevedigecip', 'pecutuqipuludoc', 'puyahocomihamog', 'qamisecuqaseciz', 'qiguhujefaceqid', 'qojivepehayuhed', 'recileqapawomoq', 'sedecijohiqelac', 'senanivugumetep', 'sijerupimenicaq', 'sizaqemezokamej', 'tehopepugibunid', 'texisijupozetar', 'topupileludirad', 'totufazilugejic', 'tubuqabadupepid', 'vafaweqarixufey', 'vajorazineriyop', 'veqirocovanasuz', 'voqazuqejuvozup', 'wagokatanikiqut', 'wemacipipocugus', 'wohapomenesiluk', 'woruwopayeqisov', 'woyugeqecedukef', 'wozifidexakehar', 'wuviwiravutajaq', 'wuyidayujelakuj', 'xebomacolalivap', 'xeceyuvaxokivur', 'xigajejabahaqik', 'xikabosekodused', 'xiqiwelujavecoz', 'xoqopehemehetiq', 'xorikumeyifekup', 'xoyayalogoragas', 'xukijohacuhozih', 'xuqexewonezaquh', 'yirayanipuseneb', 'yiwilacijihogix', 'zelenogitesavew', 'zidediwawaginat', 'zipetegosoyiyug', 'zuneyalegeyafow'] \n",
      "  \n",
      "This is the set of the first order terms: ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z']\n",
      "  \n",
      "Remaining usernames: ['botinarezofuzit', 'bufavilisiqawol', 'fokedacogasadog', 'hokayozojahumen', 'kofokuvawewomow', 'kududexalodakuw', 'kujosibahudagef', 'kuxoxiwojiquhek', 'nirilobalosoloz', 'niyajegilamikig', 'nuduvilurakawex', 'nunoqidozanetal', 'wozifidexakehar', 'wuviwiravutajaq', 'wuyidayujelakuj', 'xoqopehemehetiq', 'xorikumeyifekup', 'xoyayalogoragas', 'xukijohacuhozih', 'xuqexewonezaquh'] \n",
      "    \n",
      "Find higher order features\n",
      "  \n",
      "We will filter out [] terms before discovering 2nd order features\n",
      "  \n",
      "This is the post-filtered database: ['bacumepafomujec', 'bawituzanutigil', 'bazewavoyetuyix', 'behefitiqoqenex', 'bemopodevuyufec', 'botinarezofuzit', 'bufavilisiqawol', 'cawekizozarahod', 'cazabugucizegul', 'cufumegatacajaf', 'dizubedasenabep', 'duromijuposemul', 'favapopamepomag', 'fegeyenawulevoz', 'fegibogafizihop', 'fenuleteyutijus', 'filalibixexobur', 'fokedacogasadog', 'gamonixuruxotof', 'gijacemanodepas', 'guqogecocagilir', 'hicoqeniluhacov', 'hisefipesamijod', 'hivorukalowaxup', 'hobofevobasicav', 'hojabugiwanoful', 'hokayozojahumen', 'jobuyivetacekiy', 'jupojecajeqomex', 'kajevehaneratad', 'kiduhecowemuhuy', 'kinuwegumoxipig', 'kirafuyavegehic', 'kirikuwulirutip', 'kofokuvawewomow', 'kududexalodakuw', 'kujosibahudagef', 'kuxoxiwojiquhek', 'laraqoxugasoniq', 'lelarepekeriwim', 'lexiruducuwazod', 'liralefezilaxot', 'lizotikayiserir', 'mojexenavexijas', 'molititoqokaxit', 'namitayavifuhod', 'nasoyicahehocib', 'nedadolemanaziv', 'nenuxokaqosidih', 'nihofuyofiyaxuj', 'nirilobalosoloz', 'niyajegilamikig', 'nuduvilurakawex', 'nunoqidozanetal', 'pamigazidopupop', 'pavizobusadevez', 'paxinevedigecip', 'pecutuqipuludoc', 'puyahocomihamog', 'qamisecuqaseciz', 'qiguhujefaceqid', 'qojivepehayuhed', 'recileqapawomoq', 'sedecijohiqelac', 'senanivugumetep', 'sijerupimenicaq', 'sizaqemezokamej', 'tehopepugibunid', 'texisijupozetar', 'topupileludirad', 'totufazilugejic', 'tubuqabadupepid', 'vafaweqarixufey', 'vajorazineriyop', 'veqirocovanasuz', 'voqazuqejuvozup', 'wagokatanikiqut', 'wemacipipocugus', 'wohapomenesiluk', 'woruwopayeqisov', 'woyugeqecedukef', 'wozifidexakehar', 'wuviwiravutajaq', 'wuyidayujelakuj', 'xebomacolalivap', 'xeceyuvaxokivur', 'xigajejabahaqik', 'xikabosekodused', 'xiqiwelujavecoz', 'xoqopehemehetiq', 'xorikumeyifekup', 'xoyayalogoragas', 'xukijohacuhozih', 'xuqexewonezaquh', 'yirayanipuseneb', 'yiwilacijihogix', 'zelenogitesavew', 'zidediwawaginat', 'zipetegosoyiyug', 'zuneyalegeyafow']\n",
      "  \n",
      "    \n",
      "This is the set of the second order terms: ['ba', 'be', 'bo', 'bu', 'ca', 'cu', 'di', 'du', 'fa', 'fe', 'fi', 'fo', 'ga', 'gi', 'gu', 'hi', 'ho', 'jo', 'ju', 'ka', 'ki', 'ko', 'ku', 'la', 'le', 'li', 'mo', 'na', 'ne', 'ni', 'nu', 'pa', 'pe', 'pu', 'qa', 'qi', 'qo', 're', 'se', 'si', 'te', 'to', 'tu', 'va', 've', 'vo', 'wa', 'we', 'wo', 'wu', 'xe', 'xi', 'xo', 'xu', 'yi', 'ze', 'zi', 'zu']\n",
      "  \n",
      "Remaining usernames: [] \n",
      "  \n",
      "Congratulations! We identified all users in our database only using 1st and 2nd order terms\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tlc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bfb86a7ad001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Find higher order terms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mqu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m#########OUTPUT ALL USERS IN LEX ORDER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tlc' is not defined"
     ]
    }
   ],
   "source": [
    "remainder_database=(database)\n",
    "words=getWords(database)\n",
    "letters=getLetters(words)\n",
    "\n",
    "###########################################def getFirstOrderLetters:#########################################\n",
    "#Initialize list for 1st order combination terms\n",
    "flc=[]\n",
    "print(\"  \")\n",
    "print('Remaining usernames: {} '.format(remainder_database))\n",
    "\n",
    "for i in range(len(letters[:][0:])):\n",
    "    flc.append(letters[i][0])\n",
    "#Gets unique 1-letter combinations & sorts them is lexicographically sorted \n",
    "flc = sorted(list(set(flc)))\n",
    "print(\"  \")\n",
    "print( 'This is the set of the first order terms: {}'.format(flc))\n",
    "\n",
    "#Calls query on each 1st order combination in order and removes from the original database\n",
    "for i in range(len(flc)):\n",
    "    remainder_database=sorted(list(set(remainder_database)-set(query(flc[i]))))\n",
    "print(\"  \")\n",
    "print('Remaining usernames: {} '.format(remainder_database))\n",
    "print(\"    \")\n",
    "\n",
    "#PRINT SUCCESS MESAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "if (len(remainder_database)<1):\n",
    "    print('Congratulations! We identified all users in our database')\n",
    "else:\n",
    "    print('Find higher order features')\n",
    "\n",
    "###########################################def getSecondOrderLetters:#########################################\n",
    "\n",
    "#Creates 1st char removal list\n",
    "    remove_flc = []\n",
    "    for i in range(len(database)):\n",
    "        if (len(letters[i][0:]) < 2):\n",
    "            remove_flc.append(database[i])\n",
    "\n",
    "#filters out 1-char characters\n",
    "    remainder_database=sorted(list(set(database)-set(remove_flc))) \n",
    "    print(\"  \")\n",
    "    print(\"We will filter out {} terms before discovering 2nd order features\".format(remove_flc))\n",
    "    print(\"  \")\n",
    "    print('This is the post-filtered database: {}'.format(remainder_database))\n",
    "    print(\"  \")\n",
    "\n",
    "#Initialize list for 2nd order combination terms\n",
    "    words=getWords(remainder_database)\n",
    "    letters=getLetters(words)\n",
    "    slc=[]\n",
    "\n",
    "    for i in range(len(letters[:][0:])):\n",
    "        slc.append(letters[i][0]+letters[i][1])\n",
    "    #Gets unique 2-letter combinations & sorts them is lexicographically sorted \n",
    "    slc = sorted(list(set(slc)))\n",
    "    print(\"    \")\n",
    "    print( 'This is the set of the second order terms: {}'.format(slc))\n",
    "\n",
    "\n",
    "#Calls query on each 2nd order combination in order and removes from the original database\n",
    "    for i in range(len(slc)):\n",
    "        remainder_database=sorted(list(set(remainder_database)-set(query(slc[i]))))\n",
    "    print(\"  \")\n",
    "    print('Remaining usernames: {} '.format(remainder_database))\n",
    "    print(\"  \")\n",
    "\n",
    "#PRINT SUCCESS MESSAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "#############Check for remaining usernames; proceed to filter using third order combinations#############\n",
    "    if (len(remainder_database)<1):\n",
    "        print('Congratulations! We identified all users in our database only using 1st and 2nd order terms')\n",
    "    else:\n",
    "        print('Find higher order terms')\n",
    "###########################################def getThirdOrderLetters:#########################################\n",
    "        print(\" \")\n",
    "        print(letters)\n",
    "        #This restarts the database and allows the 3rd order feature extraction algorithm to remove 1-char and 2-char length usernames\n",
    "        remainder_database=(database)\n",
    "        words=getWords(database)\n",
    "        letters=getLetters(words)\n",
    "        print(\" \")\n",
    "        \n",
    "        #Creates 2nd char removal list\n",
    "        remove_slc = []\n",
    "        for i in range(len(database)):\n",
    "            if (len(letters[i][0:]) <3 and len(letters[i][0:]) > 1 ):\n",
    "                remove_slc.append(database[i])\n",
    "                \n",
    "        #filters out 2-char characters\n",
    "        remainder_database=sorted(list(set(database)-set(remove_slc)-set(remove_flc))) \n",
    "        print(\"  \")\n",
    "        print(\"We will filter out {} and {} terms before discovering 3rd order features\".format(remove_flc,remove_slc))\n",
    "        print(\"  \")\n",
    "        print('This is the post-filtered database: {}'.format(remainder_database))\n",
    "        print(\"  \")\n",
    "        print(\"  \")\n",
    "        \n",
    "        #Reinitialize list for 3rd order feature extraction\n",
    "        words=getWords(remainder_database)\n",
    "        letters=getLetters(words)\n",
    "        tlc=[]\n",
    "\n",
    "        for j in range(len(letters)):\n",
    "            tlc.append(letters[j][0]+letters[j][1]+letters[j][2])\n",
    "        #Gets unique 3-letter combinations & sorts them is lexicographically sorted \n",
    "        tlc = sorted(list(set(tlc)))\n",
    "        print(\"  \")\n",
    "        print( 'This is the set of the third order terms: {}'.format(tlc))\n",
    "\n",
    "        #Calls query on each 3rd order combination in order and removes from the original database\n",
    "        for i in range(len(tlc)):\n",
    "            remainder_database=sorted(list(set(remainder_database)-set(query(tlc[i]))))\n",
    "        print(\"  \")\n",
    "        print('Remaining usernames: {} '.format(remainder_database))\n",
    "        print(\"    \")\n",
    "\n",
    "\n",
    "        #PRINT SUCCESS MESSAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "        if (len(remainder_database)<1):\n",
    "            print('Congratulations! We identified all users in our database using 1st, 2nd, and 3rd order features')\n",
    "        else:\n",
    "            print('Find higher order terms')\n",
    "\n",
    "qu = len(flc)+len(slc)+len(tlc)\n",
    "\n",
    "#########OUTPUT ALL USERS IN LEX ORDER\n",
    "print(\"    \")\n",
    "print('Our features: ')\n",
    "print(\"    \")\n",
    "print(flc)\n",
    "print(\"    \")\n",
    "print(slc)\n",
    "print(\"    \")\n",
    "print(tlc)\n",
    "print(\"    \")\n",
    "print(\"    \")\n",
    "print('Number of querys to extract features: {}'.format(qu))\n",
    "print('Number of usernames: {}'.format(len(database)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Rationale Behind the Algorithm </h1>\n",
    "\n",
    "This algorithm takes in letters, or caps, A-Z, a-z, numbers, special characters -- because usernames can be anything, right? Then, it proceeds to identify lower-order features to higher-order ones, systematically decreasing the database by filtering usernames from the original database that are below the number of characters defined by a given filter. After, it uses the discovered nth-features to query the dataset on the remaining_database from the (nth-1) feature.\n",
    "\n",
    "We're calling quieries on the set of identified 1st, 2nd, 3rd, ... nth order features (characters in a word) that is unique to our database. By adding checker flags between each feature-order, we can minimize the number of characters needed to identify all the users in the database, thus minimizing the number of times we have to call query API.\n",
    "\n",
    "If there wasn't a minimum, or a maximum, on the number of characters, I think we can continue to draw out the nth order of terms to selectively identify more complex usernames and extract more complex names and sized databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>GETTING FINAL OUTPUTS</H1>\n",
    "\n",
    "The code below reconstructs the database by using nth features obtained from previous feature extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT ALL USERS IN LEX ORDER\n",
    "print(\"    \")\n",
    "print('Our features')\n",
    "print(flc)\n",
    "print(\"    \")\n",
    "print(slc)\n",
    "print(\"    \")\n",
    "print(tlc)\n",
    "\n",
    "output = []\n",
    "\n",
    "#Logic: Call query on each ith element of first order features; check if len(remaining usernames) = 0; \n",
    "#if not, continue to loop until first order features run out; if there still exists usernames, call queries on\n",
    "#each jth element of second order features; check if len(remaining_database)=0; \n",
    "#if not, continue to loop until second order featuers run out;\n",
    "\n",
    "print(\"    \")\n",
    "print(\"    \")\n",
    "\n",
    "#Rebuilding portion of our database using 1st order features\n",
    "for i in range(len(flc)):\n",
    "    output=sorted(output+query(flc[i]))\n",
    "print(\" \")\n",
    "print('This is our dataset after calling the query API with our 1st order features:')\n",
    "print(\" \")\n",
    "print(output)\n",
    "\n",
    "#Checks if output is less than the entire database; finds unique additions using 2nd order terms and rebuilds output\n",
    "if (len(output)<len(database)):\n",
    "    for j in range(len(slc)):\n",
    "        output=sorted(set(output+query(slc[j])))\n",
    "\n",
    "print(\" \")\n",
    "print('This is our dataset after calling the query API with our 2nd order features:')\n",
    "print(\" \")\n",
    "print(output)\n",
    "\n",
    "\n",
    "#Checks if output is less than the entire database; finds unique additions using 3rd order terms and rebuilds output\n",
    "if (len(output)<len(database)):\n",
    "    for k in range(len(tlc)):\n",
    "        output=sorted(set(output+query(tlc[k])))\n",
    "\n",
    "print(\" \")\n",
    "print('This is our dataset after calling the query API with our 3rd order features:')\n",
    "print(\" \")\n",
    "print(output)\n",
    "\n",
    "print(\" \")\n",
    "print('This is our original dataset:')\n",
    "print(\" \")\n",
    "print(database)\n",
    "\n",
    "assert output == sorted(database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Submission of Extract Algorithm </h1>\n",
    "\n",
    "The code below integrates the above functions and logic into the extract function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24f43aff1939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-24f43aff1939>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatabase\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-24f43aff1939>\u001b[0m in \u001b[0;36mextract\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#database = sorted(set(database))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_word' is not defined"
     ]
    }
   ],
   "source": [
    "def extract(query):\n",
    "    \"\"\"Implement this method using the `query` API call, for example:\n",
    "    query(\"abracadar\") #=> [\"abracadara\"] using the default query method in main()\n",
    "    \"\"\"\n",
    "    \n",
    "    #database = sorted(set(database))\n",
    "    database = sorted(set(database))\n",
    "\n",
    "    def getWords(data):\n",
    "        words=[]\n",
    "        for i in range(len(data)):\n",
    "            word=data[i]\n",
    "            words.append(word)\n",
    "        return words\n",
    "    \n",
    "    def getLetters(allWords):\n",
    "        #Initialize chars and Letters matrix\n",
    "        chars = []    \n",
    "        fls=[]\n",
    "        for x in range(len(allWords)):\n",
    "            listChar=list(allWords[x])\n",
    "            chars.append(listChar)   \n",
    "        #return chars\n",
    "        for i in range(len(allWords)):\n",
    "            fl=chars[i][0:]\n",
    "            fls.append(fl)\n",
    "        return fls\n",
    "    \n",
    "    remainder_database=(database)\n",
    "    words=getWords(database)\n",
    "    letters=getLetters(words)\n",
    "\n",
    "###def getFirstOrderPrefixes:###\n",
    "    #Initialize list for 1st order combination terms\n",
    "    flc=[]\n",
    "    print(\"  \")\n",
    "    print('Remaining usernames: {} '.format(remainder_database))\n",
    "\n",
    "    for i in range(len(letters[:][0:])):\n",
    "        flc.append(letters[i][0])\n",
    "    #Gets unique 1-letter combinations & sorts them is lexicographically sorted \n",
    "    flc = sorted(list(set(flc)))\n",
    "    print(\"  \")\n",
    "    print( 'This is the set of the first order terms: {}'.format(flc))\n",
    "\n",
    "    #Calls query on each 1st order combination in order and removes from the original database\n",
    "    for i in range(len(flc)):\n",
    "        remainder_database=sorted(list(set(remainder_database)-set(query(flc[i]))))\n",
    "    print(\"  \")\n",
    "    print('Remaining usernames: {} '.format(remainder_database))\n",
    "    print(\"    \")\n",
    "\n",
    "    #PRINT SUCCESS MESAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "    if (len(remainder_database)<1):\n",
    "        print('Congratulations! We identified all users in our database')\n",
    "    else:\n",
    "        print('Find higher order features')\n",
    "####def getSecondOrderLetters:###########\n",
    "        #Creates 1st char removal list\n",
    "        remove_flc = []\n",
    "        for i in range(len(database)):\n",
    "            if (len(letters[i][0:]) < 2):\n",
    "                remove_flc.append(database[i])\n",
    "\n",
    "    #filters out 1-char characters\n",
    "        remainder_database=sorted(list(set(database)-set(remove_flc))) \n",
    "        print(\"  \")\n",
    "        print(\"We will filter out {} terms before discovering 2nd order features\".format(remove_flc))\n",
    "        print(\"  \")\n",
    "        print('This is the post-filtered database: {}'.format(remainder_database))\n",
    "        print(\"  \")\n",
    "\n",
    "    #Initialize list for 2nd order combination terms\n",
    "        words=getWords(remainder_database)\n",
    "        letters=getLetters(words)\n",
    "        slc=[]\n",
    "\n",
    "        for i in range(len(letters[:][0:])):\n",
    "            slc.append(letters[i][0]+letters[i][1])\n",
    "        #Gets unique 2-letter combinations & sorts them is lexicographically sorted \n",
    "        slc = sorted(list(set(slc)))\n",
    "        print(\"    \")\n",
    "        print( 'This is the set of the second order terms: {}'.format(slc))\n",
    "\n",
    "\n",
    "    #Calls query on each 2nd order combination in order and removes from the original database\n",
    "        for i in range(len(slc)):\n",
    "            remainder_database=sorted(list(set(remainder_database)-set(query(slc[i]))))\n",
    "        print(\"  \")\n",
    "        print('Remaining usernames: {} '.format(remainder_database))\n",
    "        print(\"  \")\n",
    "\n",
    "    #PRINT SUCCESS MESSAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "    #############Check for remaining usernames; proceed to filter using third order combinations#############\n",
    "        if (len(remainder_database)<1):\n",
    "            print('Congratulations! We identified all users in our database only using 1st and 2nd order terms')\n",
    "        else:\n",
    "            print('Find higher order terms')\n",
    "    ###########################################def getThirdOrderLetters:#########################################\n",
    "\n",
    "            print(\" \")\n",
    "            print(letters)\n",
    "            #This restarts the database and allows the 3rd order feature extraction algorithm to remove 1-char and 2-char length usernames\n",
    "            remainder_database=(database)\n",
    "            words=getWords(database)\n",
    "            letters=getLetters(words)\n",
    "            print(\" \")\n",
    "        \n",
    "            #Creates 2nd char removal list\n",
    "            remove_slc = []\n",
    "            for i in range(len(database)):\n",
    "                if (len(letters[i][0:]) <3 and len(letters[i][0:]) > 1 ):\n",
    "                    remove_slc.append(database[i])\n",
    "                \n",
    "            #filters out 1-char characters\n",
    "            remainder_database=sorted(list(set(database)-set(remove_slc)-set(remove_flc))) \n",
    "            print(\"  \")\n",
    "            print(\"We will filter out {} and {} terms before discovering 3rd order features\".format(remove_flc,remove_slc))\n",
    "            print(\"  \")\n",
    "            print('This is the post-filtered database: {}'.format(remainder_database))\n",
    "            print(\"  \")\n",
    "        \n",
    "            #Reinitialize list for 3rd order feature extraction\n",
    "            words=getWords(remainder_database)\n",
    "            letters=getLetters(words)\n",
    "            tlc=[]\n",
    "\n",
    "            for j in range(len(letters)):\n",
    "                tlc.append(letters[j][0]+letters[j][1]+letters[j][2])\n",
    "            #Gets unique 3-letter combinations & sorts them is lexicographically sorted \n",
    "            tlc = sorted(list(set(tlc)))\n",
    "            print(\"  \")\n",
    "            print( 'This is the set of the third order terms: {}'.format(tlc))\n",
    "\n",
    "            #Calls query on each 3rd order combination in order and removes from the original database\n",
    "            for i in range(len(tlc)):\n",
    "                remainder_database=sorted(list(set(remainder_database)-set(query(tlc[i]))))\n",
    "            print(\"  \")\n",
    "            print('Remaining usernames: {} '.format(remainder_database))\n",
    "            print(\"    \")\n",
    "\n",
    "\n",
    "            #PRINT SUCCESS MESSAGE IF ALL USERS HAVE BEEN IDENTIFIED\n",
    "            if (len(remainder_database)<1):\n",
    "                print('Congratulations! We identified all users in our database using 1st, 2nd, and 3rd order features')\n",
    "            else:\n",
    "                print('Find higher order terms')\n",
    "\n",
    "    qu = len(flc)+len(slc)+len(tlc)\n",
    "    #########OUTPUT ALL USERS IN LEX ORDER\n",
    "\n",
    "    print(\"    \")\n",
    "    print('Our features: ')\n",
    "    print(\"    \")\n",
    "    print(flc)\n",
    "    print(\"    \")\n",
    "    print(slc)\n",
    "    print(\"    \")\n",
    "    print(tlc)\n",
    "    print(\"    \")\n",
    "    print('Number of querys to extract features: {}'.format(qu))\n",
    "    print('Number of usernames: {}'.format(len(database)))\n",
    "    \n",
    "    #OUTPUT ALL USERS IN LEX ORDER\n",
    "    print(\"    \")\n",
    "    print('Our features:')\n",
    "    print(flc)\n",
    "    print(\"    \")\n",
    "    print(slc)\n",
    "    print(\"    \")\n",
    "    print(tlc)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    #Logic: Call query on each ith element of first order features; check if len(remaining usernames) = 0; \n",
    "    #if not, continue to loop until first order features run out; if there still exists usernames, call queries on\n",
    "    #each jth element of second order features; check if len(remaining_database)=0; \n",
    "    #if not, continue to loop until second order featuers run out;\n",
    "\n",
    "    print(\"    \")\n",
    "\n",
    "    #Rebuilding portion of our database using 1st order features\n",
    "    for i in range(len(flc)):\n",
    "        output=sorted(output+query(flc[i]))\n",
    "    print(\" \")\n",
    "    print('This is our dataset after using our 1st order features to rebuild the original dataset:')\n",
    "    print(\" \")\n",
    "    print(output)\n",
    "\n",
    "    #Checks if output is less than the entire database; finds unique additions using 2nd order terms and rebuilds output\n",
    "    if (len(output)<len(database)):\n",
    "        for j in range(len(slc)):\n",
    "            output=sorted(set(output+query(slc[j])))\n",
    "\n",
    "    print(\" \")\n",
    "    print('This is our dataset after using our 2nd order features to rebuild the original dataset:')\n",
    "    print(\" \")\n",
    "    print(output)\n",
    "\n",
    "\n",
    "    #Checks if output is less than the entire database; finds unique additions using 3rd order terms and rebuilds output\n",
    "    if (len(output)<len(database)):\n",
    "        for k in range(len(tlc)):\n",
    "            output=sorted(set(output+query(tlc[k])))\n",
    "\n",
    "    print(\" \")\n",
    "    print('This is our dataset after using our 3rd order features to rebuild the original dataset:')\n",
    "    print(\" \")\n",
    "    print(output)\n",
    "\n",
    "    print(\" \")\n",
    "    print('This is our original dataset:')\n",
    "    print(\" \")\n",
    "    print(database)\n",
    "    print(\" \")\n",
    "    print('Successful extraction of all usernames in the database')\n",
    "\n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    \"\"\"Runs your solution -- no need to update (except to maybe change the database).\"\"\"\n",
    "    # Simple implementation of the autocomplete API\n",
    "    database = [\"abracadara\", \"al\", \"alice\", \"alicia\", \"allen\", \"alter\", \"altercation\", \"bob\", \"eve\", \"evening\", \"event\", \"eventually\", \"mallory\"]\n",
    "    #database=[]\n",
    "    #for i in range(10):\n",
    "    #    database.append(generate_word(15))\n",
    "    database = sorted(set(database))\n",
    "    query = lambda prefix: [d for d in database if d.startswith(prefix)][:5]\n",
    "    assert extract(query) == database\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
